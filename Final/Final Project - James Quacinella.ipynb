{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS622 Final Project: Book Recommendations\n",
    "### An Extension of Week12's Recommendation Project\n",
    "\n",
    "## Background \n",
    "\n",
    "For this final project, I will extend the work done in Week 12 where I built a recommender system for ratings data for books. As a recap on what that project was about, I used data on book ratings from [this data set](http://www2.informatik.uni-freiburg.de/~cziegler/BX/). This data set has 1 millon+ rows of user ratings (from 1 - 10) of books identified by ISBN number. \n",
    "\n",
    "I used this data to generate a recommendation system using PySpark. The model developed used matrix factorization to allow us to take in a set of ratings for a new user, and predict their ratings for any other book in our data. The conclusion from Week12, however, was that the model generated was not so great with respect to how accurate the matrix factorization reproduced the original data. \n",
    "\n",
    "One idea to improve its performance was to go through a round of de-duplication: there are books that have different ISBN numbers but are essentially the same book l(two different volumes of same book, for example). De-duplication will be done by some kind of locality sensitive hashing to determine titles that are similar enough to consider merge-worthy. I found a [Spark implementation of LSH in python](https://github.com/magsol/pyspark-lsh/) and will use it to efficently find similar books by titles.\n",
    "\n",
    "My hypothesis is that de-duplicating the data (books in this case) and merging ratings from both books into one would help the accuracy of the model. \n",
    "\n",
    "The model will be compared to Week12's results to see if the accuracy (based on a train-validation-test set split) has improved. The model will also be compared to see how much better it performs in execution time.\n",
    "\n",
    "## Data Defintion\n",
    "\n",
    "The Book-Crossing dataset comprises 3 tables.\n",
    "\n",
    "* **BX-Users:** Contains the users. Note that user IDs (`User-ID`) have been anonymized and map to integers. \n",
    "    * For our purposes, we do not nee this file\n",
    "\n",
    "\n",
    "* **BX-Books** - Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (`Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`), obtained from Amazon Web Services. \n",
    "    * We load this manually to convert ISBN numbers to an internal index number to be used in the model\n",
    "\n",
    "\n",
    "* **BX-Book-Ratings** - Contains the book rating information. Ratings (`Book-Rating`) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "    * This is the main file we load, which consists of more than one mullion rows. However, we filter out books that do not appear in the above BX-Books file (why this happens I am not sure). \n",
    "\n",
    "Git does not allow large files, so they are not in the git repo. The file 'BX-CSV-Dump.zip' can be unzipped and the data files will be loaded.\n",
    "\n",
    "## Objective\n",
    "\n",
    "My hypothesis is that de-duplicating the data (books in this case) and merging ratings from both books into one would help the accuracy of the model.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Overview\n",
    "\n",
    "* First, we will review the original model and how it was built, and associated RMSE values\n",
    "    * Create a small snippet of the data, and search for the best rank metaparameter based on validation set RMSE\n",
    "    * Use this meta parameter for the medium sized data (500000 rows) and the full data (1000000+ rows) and find the test set RMSE\n",
    "* Next we will perform de-duplication of books, merging the reviews together\n",
    "* We will then create a second model with the de-duplication in place\n",
    "* Finally, we will compare performances of these models\n",
    "\n",
    "### Recapping Week 12 - Load Data into Spark\n",
    "\n",
    "The code below loads the Book information into a Spark RDD object (this is a listing of tuples of (ISBN, title)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure the necessary Spark environment\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# We are using ALS to factor our user-to-book rating matrix\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Create PySpark context\n",
    "from pyspark import  SparkContext, SQLContext, Row\n",
    "sc = SparkContext('local[*]', '--executor-memory=8g pyspark')\n",
    "# sc.defaultParallelism\n",
    "\n",
    "book_filename = \"/spark/BX-Books.csv\"\n",
    "book_raw_data = sc.textFile(book_filename)\n",
    "book_raw_data_header = [ word.replace(\"\\\"\", \"\") for word in book_raw_data.take(1)[0].split(';')[1:3] ] # Used to skip the header\n",
    "book_raw_data =  book_raw_data.map(lambda line: [ word.replace(\"\\\"\", \"\") for word in line.split(\";\")[1:3]]) \\\n",
    "                            .filter(lambda line: line != book_raw_data_header) \\\n",
    "                            .map(lambda line: \"%s %s\" % (line[0], line[1]))\n",
    "\n",
    "# Confirm data loaded\n",
    "# book_raw_data.take(5)\n",
    "\n",
    "# Read in book data into global dict which maps isbn to title\n",
    "isbn_to_idx = { }\n",
    "isbn_idx = 0\n",
    "book_data = open(\"BX-Books.csv\", \"r\")\n",
    "for line in book_data.readlines():\n",
    "    # Split the line and filter out quotes\n",
    "    (isbn, title) = line.split(';')[0:2]\n",
    "    isbn = isbn.replace(\"\\\"\", \"\")\n",
    "    title = title.replace(\"\\\"\", \"\")\n",
    "    \n",
    "    # Store in mapping\n",
    "    isbn_to_idx[isbn] = isbn_idx\n",
    "    isbn_idx += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the Ratings information into a Spark RDD object. Ratings file is just a list of tuples of the form (user_id, isbn, rating of isbn). This is a useful format for ALS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in chosen data set (min = 5000 lines, half=50000)\n",
    "ratings_filename = \"BX-Book-Ratings.csv\"\n",
    "# ratings_filename = \"BX-Book-Ratings.half.csv\"\n",
    "# ratings_filename = 'BX-Book-Ratings.min.csv'\n",
    "ratings_raw_data = sc.textFile(ratings_filename)\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0] # Used to skip the header\n",
    "\n",
    "\n",
    "# Given a row from the file, filter out quotes and create dict of isbn -> index\n",
    "def convert_row(tokens):\n",
    "    (user_id, isbn, rating) = (tokens[0].replace(\"\\\"\", \"\"), \\\n",
    "                               tokens[1].replace(\"\\\"\", \"\"), \\\n",
    "                               tokens[2].replace(\"\\\"\", \"\"))\n",
    "    \n",
    "    # Filter out implicit ratings\n",
    "    if rating == 0:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    # Convert isbn to index and ...\n",
    "    global isbn_to_idx\n",
    "    if isbn in isbn_to_idx:\n",
    "        isbn_idx = isbn_to_idx[isbn]\n",
    "    else:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    # ... use it in ratings row for data frame\n",
    "    return (user_id, isbn_idx, rating)\n",
    "\n",
    "# Read in raw rating data and convert it to proper format\n",
    "ratings_data = ratings_raw_data \\\n",
    "                    .filter(lambda line: line != ratings_raw_data_header) \\\n",
    "                    .map(lambda line: line.split(\";\")) \\\n",
    "                    .map(lambda tokens: convert_row(tokens)) \\\n",
    "                    .filter(lambda tokens: tokens[0] is not None).cache()\n",
    "\n",
    "# Print number of ratings loaded\n",
    "print \"Loaded %d ratings\" % ratings_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the ratings data is loaded, lets create a training, validation and testing split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training, validation and test split\n",
    "training_RDD, validation_RDD, test_RDD = ratings_data.randomSplit([70, 15, 15], seed=0L)\n",
    "\n",
    "# Trim off the rating to get user-book pairs for the validation set\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Trim off the rating to get user-book pairs for the test set\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train an ALS models of various rank and pick the best one based on minimizing validation RMSE error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(training_RDD, validation_for_predict_RDD, validation_RDD):\n",
    "    # Model parameters\n",
    "    seed = 5L\n",
    "    iterations = 10\n",
    "    # iterations = 20\n",
    "    regularization_parameter = 0.1\n",
    "\n",
    "    ranks = range(20, 50, 2)\n",
    "    # ranks = [30] # Found by running on min data\n",
    "#     ranks = [36]   #  Found by running on min data, but with filtering on implicit feeback\n",
    "\n",
    "    errors = [ ]\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_iteration = -1\n",
    "    best_model = None\n",
    "\n",
    "    # Look for best ALS model iterating over diff rank values\n",
    "    for rank in ranks:\n",
    "        # Create ALS model\n",
    "        model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations, \\\n",
    "                          lambda_=regularization_parameter)\n",
    "\n",
    "        # Come up with predictions for the validation set\n",
    "        predictions = model.predictAll(validation_for_predict_RDD)\\\n",
    "                            .map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "        # TODO\n",
    "        rates_and_preds = validation_RDD\\\n",
    "                            .map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\\\n",
    "                            .join(predictions)\n",
    "\n",
    "        # Record and update the error\n",
    "        error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "        errors.append(error)\n",
    "\n",
    "        # What is the current RMSE\n",
    "        print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "\n",
    "        # Error update\n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_rank = rank\n",
    "            best_model = model\n",
    "\n",
    "    # Final output\n",
    "    print 'The best model was trained with rank %s' % best_rank\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Train model\n",
    "best_model_pre_merge = train_model(training_RDD, validation_for_predict_RDD, validation_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "----\n",
    "\n",
    "----\n",
    "\n",
    "----\n",
    "\n",
    "### Finding Titles to Merge\n",
    "\n",
    "To find the titles to merge together, we need to compare each title to all other titles in the corpus to find which ones are similar (using an appropriate distance metric, like jaccard distance). Howeverm with 200000+ books, this is not feasible. Locality sensitive hashing can help by binning titiles together that are more likely to be similar, to reduce the number of comparisons one has to do. As alluded to above, there is a [Spark implementation of LSH in python](https://github.com/magsol/pyspark-lsh/) which I will use to find book titles that are similar enough to merge. \n",
    "\n",
    "However, even with LSH, I found it necessary to use an Amazon EC2 server to get things done. Using spot instances, I setup a 16 core / 30Gb machine (or whatever was available that had som decent resources) and installed / setup ipython to use Spark. I made sure all data was stored on an EBS volume to make sure that no data was lost if the spot instance died.\n",
    "\n",
    "After that, I experimented with LSH parameters on a small subset of the data. Even with these resources, I could not process the whole set, but only 50,000 books at a time. The code in the first ipython notebook is responsible for loading the book titles into a Spark RDD object, creating an appropriate TFIDF representation of all titles, and passing the TFIDF results tothe PythonLSH module. This part of the process can take 40m to a few hours depending on the size of the dataset. I found the LSH implementation took up a lot of memory. I tried tweaking the params as best as I could, but I was still limited. See the \"Next Steps\" section below for more details.\n",
    "\n",
    "**TODO** \n",
    "\n",
    "* Link to other notebook\n",
    "* discuss issues with scaling\n",
    "\n",
    "### Merge Titles\n",
    "\n",
    "Using the results from LSH, I tend compare titles that are in each bucket, and construct a merge dictionary that maps an ISBN 'index' to an index of a title to merge it with. I serialize this dictionary to save my work as I go, and then load this in another notebook to perform the actual merging when loading ratings. As the rating data is loaded in, the ISBN is converted to an index and the dictionary is consulted to see if this so should be merged (i.e., another index is used). Therefore, all ratings for many titles for the same book will refer to one index.\n",
    "\n",
    "Here we will use the merge dictionary generated from the first 50,000 books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use spark accumulators to see how many times we merge\n",
    "count = sc.accumulator(0)\n",
    "missed_count = sc.accumulator(0)\n",
    "\n",
    "def merge_ratings(tokens):\n",
    "    global count\n",
    "    global missed_count\n",
    "    # (user_id, isbn_idx, rating)\n",
    "    orig_isbn_idx = tokens[1]\n",
    "    \n",
    "    if orig_isbn_idx in merge_dict:\n",
    "        count += 1\n",
    "        return (tokens[0], merge_dict[orig_isbn_idx], tokens[2])\n",
    "    else:\n",
    "        missed_count += 1\n",
    "        return tokens\n",
    "\n",
    "# Read in raw rating data and convert it to proper format\n",
    "new_ratings_data = ratings_raw_data \\\n",
    "                    .filter(lambda line: line != ratings_raw_data_header) \\\n",
    "                    .map(lambda line: line.split(\";\")) \\\n",
    "                    .map(lambda tokens: convert_row(tokens)) \\\n",
    "                    .filter(lambda tokens: tokens[0] is not None) \\\n",
    "                    .map(merge_ratings).cache()\n",
    "\n",
    "# Print ratings loaded\n",
    "print \"Loaded %d ratings\" % new_ratings_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Talk about lower % found with new settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count.value / float(count.value + missed_count.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data From New Post-Merged Ratings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training, validation and test split\n",
    "training_RDD, validation_RDD, test_RDD = new_ratings_data.randomSplit([70, 15, 15], seed=0L)\n",
    "\n",
    "# Trim off the rating to get user-book pairs for the validation set\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Trim off the rating to get user-book pairs for the test set\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_post_merge = train_model(training_RDD, validation_for_predict_RDD, validation_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Discussions\n",
    "\n",
    "Lets compare the pre and post-merge RMSE errors for models of different rank sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "| Pre-Merge RMSE | Post-Merge RMSE |\n",
      "+----------------+-----------------+\n",
      "| 4.26313112355  |  4.25828212238  |\n",
      "| 4.23259538535  |  4.23275657208  |\n",
      "| 4.21148751659  |  4.20943547919  |\n",
      "| 4.18826859766  |  4.18930658745  |\n",
      "| 4.16448657922  |  4.16958226847  |\n",
      "| 4.15846533055  |  4.15713268602  |\n",
      "|  4.1436364273  |   4.1416517817  |\n",
      "| 4.12220323035  |  4.12264843619  |\n",
      "| 4.11941536673  |  4.11715783255  |\n",
      "| 4.10410432488  |  4.10227015671  |\n",
      "| 4.09841351419  |  4.09726798097  |\n",
      "| 4.09207336934  |  4.08903422579  |\n",
      "| 4.08177248426  |  4.07665148098  |\n",
      "| 4.07279330036  |  4.06946764295  |\n",
      "| 4.07400818709  |  4.07036979413  |\n",
      "+----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "results = [(4.26313112355, 4.25828212238),\n",
    "         (4.23259538535, 4.23275657208),\n",
    "         (4.21148751659, 4.20943547919),\n",
    "         (4.18826859766, 4.18930658745),\n",
    "         (4.16448657922, 4.16958226847),\n",
    "         (4.15846533055, 4.15713268602),\n",
    "         (4.1436364273, 4.1416517817),\n",
    "         (4.12220323035, 4.12264843619),\n",
    "         (4.11941536673, 4.11715783255),\n",
    "         (4.10410432488, 4.10227015671),\n",
    "         (4.09841351419, 4.09726798097),\n",
    "         (4.09207336934, 4.08903422579),\n",
    "         (4.08177248426, 4.07665148098),\n",
    "         (4.07279330036, 4.06946764295),\n",
    "         (4.07400818709, 4.07036979413)]\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Pre-Merge RMSE\", \"Post-Merge RMSE\"]\n",
    "for result in results:\n",
    "    x.add_row(result)\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a huge result at all, but the RMSE is generally lower post-merge, which shows that this technique **might** have promise. The difference is pretty small, so more work would be needed to confirm that viability of this method. The results above were created from only 50,000 books (out of a total of 271,380).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The first step would be to review the LSH implementation to make sure its as efficient as possible. I took a brief look at the LSH function in the lsh.py file in the module, and it generally makes sense to me. However, the code I wrote after wards to process the results may not be the best way of doing things either (thought the majority of the time was spent in the LSH routine).\n",
    "\n",
    "After confirming the effectiveness of the module, I would expand the example to use a cluster of machines. Scaling one machine was difficult, as I had to end up tweaking per-executer memory limits (I ran a few computations that took a few hours to only die of MemoryLimit exceptions; quite frustrating!). I did not have enough time to get into this, but would have been cool to see if the LSH technique would expand easily into more Spark machines. \n",
    "\n",
    "I would also tweak the LSH parameters by either 1) a deeper understanding of the space-cpu tradeoff in the param space and maybe 2) a meta-parameter search.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "    \n",
    "The final project is designed to evaluate your synthesis of the ideas in the course on a real-world problem. While the homework exercises come from real-world problems, they have been simplified to aid the learning process. On the other hand the final project is less structured. Using data produced from your stream, build a matching system as in Chapter 8 or a recommendation system as discussed in Chapter 9. Describe the problem you are solving. Is it a vanilla problem or are there unique aspects to the problem you are solving? Characterize the volume of data and your approach using either Map Reduce or streaming methods to solve the problem. Discuss how you train the model and how you evaluate its performance. \n",
    "\n",
    "\n",
    "A 4-8 page paper must accompany working code that\n",
    "\n",
    "    describes the objective;\n",
    "    provides background on the problem;\n",
    "    discusses the methodology (source of data, size of dataset, topology of system, how data was partitioned, details of predictive model, how performance was measured);\n",
    "    discusses results (computational performance, model performance, features selected, limitations of model); and\n",
    "    concludes with potential next steps and final thoughts.\n",
    "\n",
    "Grading: The final project is graded along three axes: problem complexity, solution correctness, and exposition. Grading problem complexity and solution correctness are inversely proportional. In other words, an easy problem will be graded more strictly for correctness than a hard problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
