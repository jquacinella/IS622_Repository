{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Merge Dictionary\n",
    "\n",
    "This no tebook shows the work I did in generating a dictionary that maps one books index (in the main file) to another book, where the mapping is between equivalent titles with different ISBN numbers (which can be due to different editions, softcover versus hardcover). This I assert will help the recommendation engine, since recommendations for all editions can be merged together (this should maintain the semantics of the rating being about the book, not about a particular form of the book, to a high degree).\n",
    "\n",
    "First we setup Spark to run on all CPUs (16 in this case, which is printed out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Create PySpark context\n",
    "from pyspark import  SparkContext, SQLContext, Row\n",
    "sc = SparkContext('local[*]', '--executor-memory=10g pyspark')\n",
    "sc.defaultParallelism\n",
    "\n",
    "# stopwords_spark = sc.broadcast(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked on this iteratively, but ended up using \"books-xlarge.csv\" and \"books-xlarge2.csv\", which I created from the first 50,000 and the next 50,000 titles. I read it into an RDD object and confirm the format as a list of tuples of the form (list of tokens, idx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([u'Flight',\n",
       "   u\"Stoneman's\",\n",
       "   u'Son',\n",
       "   u'(The',\n",
       "   u'Flight',\n",
       "   u'Stoneman)',\n",
       "   u'Terence',\n",
       "   u'Munsey'],\n",
       "  0),\n",
       " ([u'Son',\n",
       "   u'Smaller',\n",
       "   u'Hero',\n",
       "   u'(New',\n",
       "   u'Canadian',\n",
       "   u'Library)',\n",
       "   u'M.',\n",
       "   u'Richler'],\n",
       "  1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_filename = \"BX-Books.csv\"\n",
    "book_filename = \"books-mini.csv\"\n",
    "book_filename = \"books-small.csv\"\n",
    "book_filename = \"books-medium.csv\"\n",
    "book_filename = \"books-large.csv\"\n",
    "book_filename = \"books-xlarge.csv\"\n",
    "book_filename = \"books-xxlarge.csv\"\n",
    "# book_filename = \"books-xlarge-tail.csv\"\n",
    "book_filename = \"books-xlarge2.csv\"\n",
    "\n",
    "book_raw_data = sc.textFile(book_filename, minPartitions=6)\n",
    "book_raw_data_header = [ word.replace(\"\\\"\", \"\") for word in book_raw_data.take(1)[0].split(';')[1:3] ] # Used to skip the header\n",
    "book_raw_data =  book_raw_data.map(lambda line: [ word.replace(\"\\\"\", \"\") for word in line.split(\";\")[1:3]]) \\\n",
    "                            .filter(lambda line: line != book_raw_data_header) \\\n",
    "                            .map(lambda line: \"%s %s\" % (line[0], line[1])) \\\n",
    "                            .map(lambda line: [word for word in line.split(\" \") if word.lower() not in stopwords]).zipWithIndex()\n",
    "book_raw_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a SQL context around this to allow for easy lookups of books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "book_table = book_raw_data.map(lambda p: Row(idx=p[1], title=p[0]))\n",
    "schema_books = sqlContext.createDataFrame(book_table)\n",
    "schema_books.registerTempTable(\"books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are examples of how to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(idx=400, title=[u'Mouth', u'Mouth', u'Kevin', u'Elyot'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "book = sqlContext.sql(\"SELECT * FROM books WHERE idx = 400 LIMIT 1\")\n",
    "book.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Flight',\n",
       "  u\"Stoneman's\",\n",
       "  u'Son',\n",
       "  u'(The',\n",
       "  u'Flight',\n",
       "  u'Stoneman)',\n",
       "  u'Terence',\n",
       "  u'Munsey']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = sqlContext.sql(\"SELECT title FROM books\").map(lambda row: row[0])\n",
    "titles.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I take all the titles and create a TFIDF representation using MLLib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(titles)\n",
    "from pyspark.mllib.feature import IDF\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0401930809\n"
     ]
    }
   ],
   "source": [
    "print end - start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using that, I send the TFIDF representation through the LSH python module, using a certain set of parameters. This also took quite a lot of tweaking to get right, and to understand how the params affect runtime cpu and memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp 1\n",
      "6\n",
      "bp 2\n",
      "6\n",
      "bp 3\n",
      "bp 4\n",
      "6\n",
      "bp 5\n",
      "6\n",
      "bp 6\n",
      "12\n",
      "bp 7\n",
      "12\n",
      "bp 8\n",
      "24.3293390274\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from pyspark_lsh import lsh\n",
    "# import pyspark_lsh\n",
    "# reload(pyspark_lsh)\n",
    "\n",
    "# p : integer, larger than the largest value in data.\n",
    "# m : integer, number of bins for hashing.\n",
    "# n : integer, number of rows to split the signatures into.\n",
    "# b : integer, number of bands. Each band will have (n / b) element\n",
    "# c : integer, minimum allowable cluster size.\n",
    "lsh_model = lsh.run(tfidf, p = 100, m = 100, n = 50, b = 10, c = 5)\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the LSH model scores RDD, which maps a bucket to a 'score', which is an average Jaccard distance of its members. I do some more filtering to help reduce the size of the results. I print out the number of buckets to check and how long it took, and how many partitions the results are spead over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "buckets_to_check = lsh_model.scores.filter(lambda bucket_score: bucket_score[1] > 200).collect()\n",
    "# print buckets_to_check\n",
    "print len(buckets_to_check)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2221.714535\n"
     ]
    }
   ],
   "source": [
    "print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_model.scores.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I setup some useful functions to implement a [disjoint data structure](https://en.wikipedia.org/wiki/Disjoint-set_data_structure), which I use to merge books together to a single book (i.e. if b is mapped to a, and c is mapped to be, a useful merge would be to map both b and c to a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.quora.com/How-do-you-implement-a-Disjoint-set-data-structure-in-Python\n",
    "global_merge_list = {}\n",
    "\n",
    "def parent(rep, v):\n",
    "    if rep[v] == v:\n",
    "        return v\n",
    "    rep[v] = parent(rep, rep[v])\n",
    "    return rep[v]\n",
    "\n",
    "def merge(rep, L):\n",
    "    for edge in L:\n",
    "        u, v = edge\n",
    "        if u not in rep:\n",
    "            rep[u] = u\n",
    "        if v not in rep:\n",
    "            rep[v] = v\n",
    "        rep[parent(rep, v)] = parent(rep, u)\n",
    "    return rep\n",
    "\n",
    "def jaccard(a, b):\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each bucket, compare all titles to one another (only half-cartesian) and if the Jaccard distance is > than 0.85, I consider it a candidate for merging and add it to the global dictionary (and use the functions above to get the correct location to place the merge). I print out how long each bcket evaluation takes, which caused too much output (on average, it took about 0.7s) so I cleared it from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for (bucket_idx, score) in buckets_to_check:\n",
    "    bstart = time.time()\n",
    "    \n",
    "    bv = lsh_model.buckets_vectors.filter(lambda bv: bv[0] == bucket_idx)\n",
    "    # print bv.collect()\n",
    "\n",
    "    values = bv.groupByKey().collect()[0][1]\n",
    "    \n",
    "#     books_in_bucket = []\n",
    "#     for x in bv.collect():\n",
    "#         books_in_bucket.append(sqlContext.sql(\"select * from books where idx = %s LIMIT 1\" % x[1]) \\\n",
    "#                                         .map(lambda row: (row[0], row[1])).collect()[0])\n",
    "    books_in_bucket = sqlContext.sql(\"select * from books where idx in %s\" % str(values.data).replace('[', '(').replace('L', '').replace(']', ')')) \\\n",
    "                                  .map(lambda row: (row[0], row[1])).collect()\n",
    "    \n",
    "    to_merge = []\n",
    "    for idx1, book1 in enumerate(books_in_bucket):\n",
    "        (gidx1, title1) = book1\n",
    "        for idx2 in range(idx1+1, len(books_in_bucket)):\n",
    "            book2 = books_in_bucket[idx2]\n",
    "            (gidx2, title2) = book2\n",
    "\n",
    "            x = set(title1)\n",
    "            y = set(title2)\n",
    "\n",
    "            if jaccard(x,y) > 0.85:\n",
    "                to_merge.append((gidx1, gidx2))\n",
    "    \n",
    "    #pprint(to_merge)\n",
    "    \n",
    "    merge(global_merge_list, to_merge)\n",
    "    \n",
    "    print(time.time() - bstart)\n",
    "\n",
    "#pprint(global_merge_list)\n",
    "\n",
    "end = time.time()\n",
    "print end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a section of the results and see if they are indeed good matches for merging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(idx=8192, title=[u\"Fortune's\", u'Wheel', u'Cynthia', u'Voigt'])]\n",
      "[Row(idx=3176, title=[u\"Fortune's\", u'Wheel', u'Cynthia', u'Voigt'])]\n",
      "\n",
      "[Row(idx=24578, title=[u'Silicon', u'Snake', u'Oil:', u'Second', u'Thoughts', u'Information', u'Highway', u'Clifford', u'Stoll'])]\n",
      "[Row(idx=17567, title=[u'Silicon', u'Snake', u'Oil:', u'Second', u'Thoughts', u'Information', u'Highway', u'Clifford', u'Stoll'])]\n",
      "\n",
      "[Row(idx=8199, title=[u'Secret', u'Garden', u'(Penguin', u'Classics)', u'Frances', u'Hodgson', u'Burnett'])]\n",
      "[Row(idx=1145, title=[u'Secret', u'Garden', u'(Penguin', u'Popular', u'Classics)', u'Frances', u'Hodgson', u'Burnett'])]\n",
      "\n",
      "[Row(idx=47788, title=[u'Stones', u'Summer', u'Dow', u'Mossman'])]\n",
      "[Row(idx=4610, title=[u'Stones', u'Summer', u'Dow', u'Mossman'])]\n",
      "\n",
      "[Row(idx=24612, title=[u'Tan', u'Veloz', u'Como', u'El', u'Deseo', u'Laura', u'Esquivel'])]\n",
      "[Row(idx=18724, title=[u'Tan', u'Veloz', u'Como', u'El', u'Deseo', u'Laura', u'Esquivel'])]\n",
      "\n",
      "[Row(idx=24619, title=[u'Unholy', u'Fire', u'Whitley', u'Strieber'])]\n",
      "[Row(idx=62, title=[u'Unholy', u'Fire', u'Whitley', u'Strieber'])]\n",
      "\n",
      "[Row(idx=16432, title=[u'Zen', u'Art', u'Motorcycle', u'Maintenance:', u'Inquiry', u'Values', u'Robert', u'M.', u'Pirsig'])]\n",
      "[Row(idx=4483, title=[u'Zen', u'Art', u'Motorcycle', u'Maintenance:', u'Inquiry', u'Values', u'Robert', u'M.', u'Pirsig'])]\n",
      "\n",
      "[Row(idx=32833, title=[u'Dracula', u'Bram', u'Stoker'])]\n",
      "[Row(idx=2119, title=[u'Dracula', u'Bram', u'Stoker'])]\n",
      "\n",
      "[Row(idx=24645, title=[u'Arrowsmith', u'(Signet', u'Classics', u'(Paperback))', u'Sinclair', u'Lewis'])]\n",
      "[Row(idx=24500, title=[u'Arrowsmith', u'(Signet', u'Classics', u'(Paperback))', u'Sinclair', u'Lewis'])]\n",
      "\n",
      "[Row(idx=45069, title=[u'BITCH', u'Judy', u'Collins'])]\n",
      "[Row(idx=6275, title=[u'BITCH', u'Judy', u'Collins'])]\n",
      "\n",
      "[Row(idx=32854, title=[u'Return', u'King', u'(The', u'Lord', u'Rings,', u'Part', u'3)', u'J.', u'R.', u'R.', u'Tolkien'])]\n",
      "[Row(idx=1971, title=[u'Return', u'King', u'(The', u'Lord', u'Rings,', u'Part', u'3)', u'J.', u'R.', u'R.', u'Tolkien'])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for merge1, merge2 in global_merge_list.iteritems():\n",
    "    if merge1 != merge2:\n",
    "        book = sqlContext.sql(\"SELECT * FROM books WHERE idx = %s LIMIT 1\" % merge1)\n",
    "        print book.collect()\n",
    "        book = sqlContext.sql(\"SELECT * FROM books WHERE idx = %s LIMIT 1\" % merge2)\n",
    "        print book.collect()\n",
    "        print\n",
    "        count += 1\n",
    "        if count > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the merge dictionary to the disk so if the spot instance dies, I have saved work. From here, I use this in the main notebook to perform the actual merging. \n",
    "\n",
    "In total, I generated *merge_dict_v1_p100m100n50b10c5_xlarge* and *merge_dict_v1_p100m100n50b10c5_xlarge2*, which are generated from the first 50,000 and the next 50,000 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open('merge_dict_v1_p100m100n50b10c5_xlarge2', 'wb')\n",
    "pickle.dump(global_merge_list, f, protocol=2)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
