out <- wordcount(hdfs.data, hdfs.out)
## Fetch results from HDFS
results <- from.dfs(out)
## check top 30 frequent words
results.df <- as.data.frame(results, stringsAsFactors=F)
colnames(results.df) <- c('word', 'count')
head(results.df[order(results.df$count, decreasing=T), ], 30)
library(rmr2)
## map function
map <- function(k,lines) {
words.list <- strsplit(lines, '\\s')
words <- unlist(words.list)
return( keyval(words, 1) )
}
## reduce function
reduce <- function(word, counts) {
keyval(word, sum(counts))
}
wordcount <- function (input, output=NULL) {
mapreduce(input=input, output=output, input.format="text",
map=map, reduce=reduce)
}
## delete previous result if any
system("/Users/hadoop/hadoop-1.1.2/bin/hadoop fs -rmr wordcount/out")
## Submit job
hdfs.root <- 'wordcount'
hdfs.data <- file.path(hdfs.root, 'data')
hdfs.out <- file.path(hdfs.root, 'out')
out <- wordcount(hdfs.data, hdfs.out)
## Fetch results from HDFS
results <- from.dfs(out)
## check top 30 frequent words
results.df <- as.data.frame(results, stringsAsFactors=F)
colnames(results.df) <- c('word', 'count')
head(results.df[order(results.df$count, decreasing=T), ], 30)
library(rmr2)
## map function
map <- function(k,lines) {
words.list <- strsplit(lines, '\\s')
words <- unlist(words.list)
return( keyval(words, 1) )
}
## reduce function
reduce <- function(word, counts) {
keyval(word, sum(counts))
}
wordcount <- function (input, output=NULL) {
mapreduce(input=input, output=output, input.format="text",
map=map, reduce=reduce)
}
## delete previous result if any
system("/usr/local/hadoop/bin/hadoop fs -rmr wordcount/out")
## Submit job
hdfs.root <- 'wordcount'
hdfs.data <- file.path(hdfs.root, 'data')
hdfs.out <- file.path(hdfs.root, 'out')
out <- wordcount(hdfs.data, hdfs.out)
## Fetch results from HDFS
results <- from.dfs(out)
## check top 30 frequent words
results.df <- as.data.frame(results, stringsAsFactors=F)
colnames(results.df) <- c('word', 'count')
head(results.df[order(results.df$count, decreasing=T), ], 30)
library(rmr2)
## map function
map <- function(k,lines) {
words.list <- strsplit(lines, '\\s')
words <- unlist(words.list)
return( keyval(words, 1) )
}
## reduce function
reduce <- function(word, counts) {
keyval(word, sum(counts))
}
wordcount <- function (input, output=NULL) {
mapreduce(input=input, output=output, input.format="text",
map=map, reduce=reduce)
}
## delete previous result if any
system("/usr/local/hadoop/bin/hadoop fs -rmr wordcount/out")
## Submit job
hdfs.root <- '/wordcount'
hdfs.data <- file.path(hdfs.root, 'data')
hdfs.out <- file.path(hdfs.root, 'out')
out <- wordcount(hdfs.data, hdfs.out)
## Fetch results from HDFS
results <- from.dfs(out)
## check top 30 frequent words
results.df <- as.data.frame(results, stringsAsFactors=F)
colnames(results.df) <- c('word', 'count')
head(results.df[order(results.df$count, decreasing=T), ], 30)
library(rmr2)
library(rhdfs)
ints = to.dfs(1:100)
calc = mapreduce(input = ints,
map = function(k, v) cbind(v, 2*v))
library(rmr2)
library(rhdfs)
ints = to.dfs(1:50)
calc = mapreduce(input = ints,
map = function(k, v) cbind(v, 2*v))
library(rmr2)
library(rhdfs)
hdfs.init()
ints = to.dfs(1:50)
calc = mapreduce(input = ints,
map = function(k, v) cbind(v, 2*v))
hdfs.ls
("/")
hdfs.ls("/")
install.packages("quickcheck")
install.packages("ravro")
from.dfs(to.dfs(1:100))
from.dfs(mapreduce(to.dfs(1:100)))
from.dfs(mapreduce(to.dfs(1:100)))
from.dfs(mapreduce(to.dfs(1:100)))
Original formaula: $5x10^17x5x10^5x10^-18 = 250000$
5 * 10^17 * 2 * 10^6 * 10^-18
2*10^18*5*10^5*10^-20
5*10^17*1.7*10^9*10^-18
1 + (1/10) + ((1/10)^2)/2 + ((1/10)^3)/6
1 + 2 + (2)^2/2 + (2)^3/6 + (2)^4/24
2^5 / 120
2^6 / 720
2^3/6
1 + 2 + 2 + 1.33333 + 0.2666667 + 0.08888889
2^7/factorial(7)
2^8/factorial(8)
1 + 2 + 2 + 1.33333 + 0.2666667 + 0.08888889 + 0.02539683 + 0.006349206
(2^2)/factorial(2)
(2^3)/factorial(3)
(2^4)/factorial(4)
(2^5)/factorial(5)
(2^6)/factorial(6)
(2^7)/factorial(7)
(2^8)/factorial(8)
1 + 2 + 2 + 1.33333 + 0.6666667 + 0.2666667 + 0.08888889 + 0.02539683 + 0.006349206
log2(10^7/320)
library("rmr2")
library(rhdfs)
hdfs.init()
ints = to.dfs(1:50)
library(rmr2)
ints = to.dfs(1:50)
from.dfs(mapreduce(to.dfs(1:100)))
library(rmr2)
library(rhdfs)
hdfs.init()
ints = to.dfs(1:50)
from.dfs(mapreduce(to.dfs(1:100)))
library(rhdfs)
library(rmr2)
hdfs.init()
from.dfs(mapreduce(to.dfs(1:100)))
from.dfs(mapreduce(to.dfs(1:100)))
library(rmr2)
library(rhdfs)
hdfs.init()
from.dfs(mapreduce(to.dfs(1:100)))
from.dfs(mapreduce(to.dfs(1:100)))
library(rJava)
library(rhdfs)
# Initialise
hdfs.init()
library(rmr2)
a <- to.dfs(seq(from=1, to=500, by=3), output="test")
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
library(rJava)
library(rhdfs)
# Initialise
hdfs.init()
library(rmr2)
a <- to.dfs(seq(from=1, to=500, by=3), output="/test")
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
library(functional)
install.packages('functional', lib="/usr/local/lib/R/site-library")
install.packages("functional", lib = "/usr/local/lib/R/site-library")
.libPaths()
system.file(package="functional")
install.packages("functional", "/usr/local/lib/R/site-library")
install.packages("functional", "/usr/local/lib/R/site-library")
sudo install.packages("functional", "/usr/local/lib/R/site-library")
install.packages("functional", "/usr/local/lib/R/site-library")
install.packages("functional", "/usr/local/lib/R/site-library")
library(rJava)
library(rhdfs)
# Initialise
hdfs.init()
library(rmr2)
a <- to.dfs(seq(from=1, to=500, by=3), output="/test")
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
library(rJava)
library(rhdfs)
# Initialise
hdfs.init()
library(rmr2)
a <- to.dfs(seq(from=1, to=500, by=3), output="/test")
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest",
"functional", "stringr", "plyr", "reshape2", "dplyr",
"R.methodsS3", "caTools", "Hmisc"), "/usr/local/lib/R/site-library")
install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest",
"functional", "stringr", "plyr", "reshape2", "dplyr",
+                    "R.methodsS3", "caTools", "Hmisc"), "/usr/local/lib/R/site-library")
"functional", "stringr", "plyr", "reshape2", "dplyr",
"R.methodsS3", "caTools", "Hmisc"), "/usr/local/lib/R/site-library")
+                    "R.methodsS3", "caTools", "Hmisc"), "/usr/local/lib/R/site-library")l
install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest", "functional", "stringr", "plyr", "reshape2", "dplyr", "R.methodsS3", "caTools", "Hmisc"), lib="/usr/local/lib/R/site-library")
install.packages(c("rJava", "Rcpp", "RJSONIO", "bitops", "digest",
"functional", "stringr", "plyr", "reshape2", "dplyr", "R.methodsS3", "caTools", "Hmisc"), lib="/usr/local/lib/R/site-library")
library(rJava)
library(rhdfs)
# Initialise
hdfs.init()
library(rmr2)
a <- to.dfs(seq(from=1, to=500, by=3), output="/test")
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
"functional", "stringr", "plyr", "reshape2", "dplyr", "R.methodsS3", "caTools", "Hmisc")install.packages(c("memoise", "rjson"), , lib="/usr/local/lib/R/site-library")
install.packages(c("memoise", "rjson"), lib="/usr/local/lib/R/site-library")
library(rJava)
library(rhdfs)
hdfs.init()
library(rmr2)
a <- to.dfs(seq(from=1, to=500, by=3), output="/test")
a <- to.dfs(seq(from=1, to=500, by=3), output="/test")
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
b <- mapreduce(input=a, map=function(k,v){keyval(v,v*v)})
.libPaths()
library(rJava)
library(rhdfs)
hdfs.init()
library(rmr2)
groups = to.dfs(groups)
from.dfs(
mapreduce(
input = groups,
map = function(., v) keyval(v, 1),
reduce =
function(k, vv)
keyval(k, length(vv))))
groups = rbinom(32, n = 50, prob = 0.4)
groups
tapply(groups, groups, length)
groups = to.dfs(groups)
?createDataFrame
# Copy points from figure
points <- matrix(c(2, 2,
3, 4,
5, 2,
4, 10,
7, 10,
4, 8,
6, 8,
10, 5,
12, 6,
11, 4,
9, 3,
12, 3), byrow=TRUE, nrow=12)
points
dist
dist()
?dist
dist(points)
dist(points, upper=TRUE)
sum
?sum
apply(dist(points, upper=TRUE), 1, sum)
?apply
dist(points, upper=TRUE, diag=TRUE)
apply(dist(points, upper=TRUE, diag=TRUE), 1, sum)
dim(dist(points, upper=TRUE, diag=TRUE))
dim(as.matrix(dist(points, upper=TRUE, diag=TRUE)))
apply( as.matrix(dist(points, upper=TRUE, diag=TRUE)), 1, sum)
whihc.min(apply( as.matrix(dist(points, upper=TRUE, diag=TRUE)), 1, sum))
which.min(apply( as.matrix(dist(points, upper=TRUE, diag=TRUE)), 1, sum))
points
points[which.min(apply( as.matrix(dist(points, upper=TRUE, diag=TRUE)), 1, sum)), ]
# Centroid is the point minimizing distance to all other points in cluster
points[which.min(apply( as.matrix(dist(points, upper=TRUE, diag=TRUE)), 1, sum)), ]
dist(points, upper=TRUE, diag=TRUE)
dist(points, upper=TRUE, diag=TRUE)**2
2.23**2
?squre
?square
?**
2.23.^2
dist(points, upper=TRUE, diag=TRUE).^2
dist(points, upper=TRUE, diag=TRUE) .^ 2
dist(points, upper=TRUE, diag=TRUE) ** 2
sapply(dist(points, upper=TRUE, diag=TRUE), function(x) x^2)
distances <- dist(points, upper=TRUE, diag=TRUE)
distances*distances
distances
(3.5)^2
distances <- as.matrix(dist(points, upper=TRUE, diag=TRUE))
distances*distances
distances
distances^
2
# Centroid is the point minimizing distance^2 to all other points in cluster
distances <- as.matrix(dist(points, upper=TRUE, diag=TRUE))^2
points[which.min(apply( distances, 1, sum)), ]
which.min(apply( distances, 1, sum))
# Centroid is the point minimizing distance^2 to all other points in cluster
distances <- as.matrix(dist(points, upper=TRUE, diag=TRUE))^2
clustroid_idx <- which.min(apply( distances, 1, sum))
points[clustroid_idx, ]
distances
clustroid_idx
apply( distances, 1, sum)
which.min(apply( distances, 1, sum))
?which.min
which.min(apply( distances, 1, sum))[2]
which.min(apply( distances, 1, sum))[1]
class(which.min(apply( distances, 1, sum)))
which.min(apply( distances, 1, sum))[1, ]
which.min(apply( distances, 1, sum))
points[clustroid_idx, ]
distances[clustroid_idx]
distances[clustroid_idx, ]
sum(distances[clustroid_idx, ])
distances
distances[5, ]
distances[6, ]
points[5, ]
points[6, ]
distances
points[5, ]
points[7, ]
distances
points[12, ] # 7,10
points[1, ] # 4,8
points[12, ] # 7,10
points[1, ] # 4,8
europe
data(europe)
?europe
library(datasets)
data(europe)
read.csv('europe.csv')
getwd()
setwd("/home/james/Code/Masters//IS622")
setwd("/home/james/Code/Masters//IS622/Week9/")
read.csv('europe.csv')
europe <- read.csv('europe.csv')
setwd("/home/james/Code/Masters//IS622/Week9/")
europe <- read.csv('europe.csv')
europe
library(rmr2)
## @knitr kmeans-signature
kmeans.mr =
function(
P,
num.clusters,
num.iter,
combine,
in.memory.combine) {
## @knitr kmeans-dist.fun
dist.fun =
function(C, P) {
apply(
C,
1,
function(x)
colSums((t(P) - x)^2))}
## @knitr kmeans.map
kmeans.map =
function(., P) {
nearest = {
if(is.null(C))
sample(
1:num.clusters,
nrow(P),
replace = TRUE)
else {
D = dist.fun(C, P)
nearest = max.col(-D)}}
if(!(combine || in.memory.combine))
keyval(nearest, P)
else
keyval(nearest, cbind(1, P))}
## @knitr kmeans.reduce
kmeans.reduce = {
if (!(combine || in.memory.combine) )
function(., P)
t(as.matrix(apply(P, 2, mean)))
else
function(k, P)
keyval(
k,
t(as.matrix(apply(P, 2, sum))))}
## @knitr kmeans-main-1
C = NULL
for(i in 1:num.iter ) {
C =
values(
from.dfs(
mapreduce(
P,
map = kmeans.map,
reduce = kmeans.reduce)))
if(combine || in.memory.combine)
C = C[, -1]/C[, 1]
## @knitr end
#      points(C, col = i + 1, pch = 19)
## @knitr kmeans-main-2
if(nrow(C) < num.clusters) {
C =
rbind(
C,
matrix(
rnorm(
(num.clusters -
nrow(C)) * nrow(C)),
ncol = nrow(C)) %*% C) }}
C}
## @knitr end
## sample runs
##
out = list()
for(be in c("local")) {
rmr.options(backend = be)
set.seed(0)
## @knitr kmeans-data
P =
do.call(
rbind,
rep(
list(
matrix(
rnorm(10, sd = 10),
ncol=2)),
20)) +
matrix(rnorm(200), ncol =2)
## @knitr end
#  x11()
#  plot(P)
#  points(P)
out[[be]] =
## @knitr kmeans-run
kmeans.mr(
to.dfs(P),
num.clusters = 12,
num.iter = 5,
combine = FALSE,
in.memory.combine = FALSE)
## @knitr end
}
P
out
library(kmeans)
library(stats)
kmeans(P, 12)
model.kmeans.builtin <- kmeans(P, 12)
model.kmeans.builtin$centers
?sort
sort(model.kmeans.builtin$centers)
model.kmeans.builtin$centers[order(model.kmeans.builtin$centers$1),]
model.kmeans.builtin$centers[order(model.kmeans.builtin$centers[,1]),]
out
out$local[order(out$local[,1]), ]
model.kmeans.builtin$centers[order(model.kmeans.builtin$centers[,1]),]
model.kmeans.builtin <- kmeans(P, 12)
model.kmeans.builtin$centers[order(model.kmeans.builtin$centers[,1]),]
out$local[order(out$local[,1]), ]
library(ggplot2)
ggplot(data=out$local) + geom_scatter()
as.dataframe(out$local)
as.data.frame(out$local)
ggplot(data=oas.data.frame(out$local), aes(x=V1, y=V2)) + geom_scatter()
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2)) + geom_scatter()
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2)) + geom_plot()
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2)) + geom_point()
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2)) + geom_point() + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2))
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2)) + geom_point() + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour=red))
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2)) + geom_point() + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red'))
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2, colour='red')) + geom_point() + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red'))
ggplot(data=as.data.frame(out$local), aes(x=V1, y=V2, colour='black')) + geom_point() + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red'))
ggplot(data=as.data.frame(out$local)) + geom_point( aes(x=V1, y=V2, colour='black')) + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red'))
ggplot(data=as.data.frame(out$local)) + geom_point( aes(x=V1, y=V2, colour='blue')) + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red'))
ggplot(data=as.data.frame(out$local)) + geom_point(aes(x=V1, y=V2, colour='blue')) + geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red'))
library(ggplot2)
ggplot(data=as.data.frame(out$local)) +
geom_point(aes(x=V1, y=V2, colour='blue')) +
geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red')) +
scale_color_manual(values = c("red", "black"),
labels = c("Distribution 1",
"Distribution 2"))
"KMeans Stats Module"))
library(ggplot2)
ggplot(data=as.data.frame(out$local)) +
geom_point(aes(x=V1, y=V2, colour='blue')) +
geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red')) +
scale_color_manual(values = c("red", "black"),
labels = c("KMeans Hadoop",
"KMeans Stats Module"))
P
library(ggplot2)
ggplot(data=as.data.frame(out$local)) +
geom_point(aes(x=V1, y=V2, colour='blue')) +
geom_point(data = as.data.frame(model.kmeans.builtin$centers), aes(x=V1, y=V2, colour='red')) +
scale_color_manual(values = c("red", "black"),
labels = c("KMeans Hadoop",
"KMeans Stats Module")) +
ggtitle("KMeans Centers for P") +
xlab("X-coord") + ylab("Y-coord")
?t
library(rJava)
library(rhdfs)
