{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Clustering with Spark\n",
    "\n",
    "Used this as a guide on how to setup PySpark and IPython notebook integration: https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python\n",
    "\n",
    "## Intro and Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark, master=local) created by __init__ at <ipython-input-10-63c0a54bcc1b>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-0416763df764>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create PySpark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'local'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pyspark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/james/Software/spark-1.5.1-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m/home/james/Software/spark-1.5.1-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway)\u001b[0m\n\u001b[0;32m    248\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 250\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    251\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark, master=local) created by __init__ at <ipython-input-10-63c0a54bcc1b>:2 "
     ]
    }
   ],
   "source": [
    "# Create PySpark context\n",
    "from pyspark import  SparkContext\n",
    "sc = SparkContext( 'local', 'pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n"
     ]
    }
   ],
   "source": [
    "# Simple example of distributed computation of the number of primes till 1000000\n",
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "nums = sc.parallelize(xrange(1000000))\n",
    "\n",
    "# Compute the number of primes in the RDD\n",
    "print nums.filter(isprime).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[170] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "# Load in shakespear text file\n",
    "text = sc.textFile(\"shakespeare.txt\")\n",
    "print text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[171] at RDD at PythonRDD.scala:43\n"
     ]
    }
   ],
   "source": [
    "# Define function to take in string an return list of tokens. Could use nltk here\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Use function above and map it to every entry in the RDD\n",
    "words = text.flatMap(tokenize)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[172] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[170] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      " |  shakespeare.txt HadoopRDD[169] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "# Map each word to a tuple of (word, 1) to indiate that this word appeared once\n",
    "wc = words.map(lambda x: (x,1))\n",
    "print wc.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[177] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[176] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[175] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(1) PairwiseRDD[174] at reduceByKey at <ipython-input-60-f50e20538e62>:3 []\n",
      "    |  PythonRDD[173] at reduceByKey at <ipython-input-60-f50e20538e62>:3 []\n",
      "    |  MapPartitionsRDD[170] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "    |  shakespeare.txt HadoopRDD[169] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "# Reduce the generated keys, using add to add the resulting values together for every key\n",
    "from operator import add\n",
    "counts = wc.reduceByKey(add)\n",
    "print counts.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6076\r\n",
      "drwxrwxr-x 2 james james    4096 Nov  1 14:52 .\r\n",
      "drwxrwxr-x 5 james james    4096 Nov  1 14:51 ..\r\n",
      "-rw-r--r-- 1 james james 6159858 Nov  1 14:52 part-00000\r\n",
      "-rw-rw-r-- 1 james james   48132 Nov  1 14:52 .part-00000.crc\r\n",
      "-rw-r--r-- 1 james james       0 Nov  1 14:52 _SUCCESS\r\n",
      "-rw-rw-r-- 1 james james       8 Nov  1 14:52 ._SUCCESS.crc\r\n"
     ]
    }
   ],
   "source": [
    "# Save counts to folder wc, and look into output folder\n",
    "counts.saveAsTextFile(\"wc\")\n",
    "!ls -al wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'kingrichardiii@18311', 1)\r\n",
      "(u'troilusandcressida@83747', 1)\r\n",
      "(u'considered.', 2)\r\n",
      "(u'kinghenryviii@7731', 1)\r\n",
      "(u'hamlet@141843', 1)\r\n",
      "(u'othello@36737', 1)\r\n",
      "(u'kinghenryviii@7732', 1)\r\n",
      "(u'othello@36738', 1)\r\n",
      "(u'romeoandjuliet@1862', 1)\r\n",
      "(u'coriolanus@166868', 1)\r\n"
     ]
    }
   ],
   "source": [
    "# Show output from word count and stop the spark context\n",
    "!head wc/part-00000\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with K-Means Example\n",
    "\n",
    "Follow the example from http://spark.apache.org/docs/latest/mllib-clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 0.692820323028\n"
     ]
    }
   ],
   "source": [
    "from pyspark import  SparkContext\n",
    "sc = SparkContext('local', 'pyspark')\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"%s/data/mllib/kmeans_data.txt\" % os.environ['SPARK_HOME'])\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10,\n",
    "        runs=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"myModelPath\")\n",
    "sameModel = KMeansModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_clusters(clusters):\n",
    "    ''' Show cluster centers '''\n",
    "    print(\"Cluster centers: \")\n",
    "    pprint(clusters.centers)\n",
    "\n",
    "def show_predictions(examples):\n",
    "    ''' Show we can predict new arrays '''\n",
    "    for example in examples:\n",
    "        print(\"%s is clustered to: %s\" % (example, clusters.centers[clusters.predict(example)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers: \n",
      "[array([ 0.1,  0.1,  0.1]), array([ 9.1,  9.1,  9.1])]\n",
      "[1 1 1] is clustered to: [ 0.1  0.1  0.1]\n",
      "[8 8 8] is clustered to: [ 9.1  9.1  9.1]\n"
     ]
    }
   ],
   "source": [
    "show_clusters(clusters)\n",
    "show_predictions([ array([1, 1, 1]), array([8, 8, 8]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means for Mini Project\n",
    "\n",
    "Lets do the above but for the data from previous weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.08,  0.08,  0.1 ,  0.24,  0.9 ]),\n",
       " array([ 0.06,  0.06,  0.05,  0.25,  0.33]),\n",
       " array([ 0.1 ,  0.1 ,  0.15,  0.65,  0.3 ]),\n",
       " array([ 0.08,  0.08,  0.08,  0.98,  0.24]),\n",
       " array([ 0.09,  0.15,  0.4 ,  0.1 ,  0.66]),\n",
       " array([ 0.1 ,  0.1 ,  0.43,  0.29,  0.56]),\n",
       " array([ 0.15,  0.02,  0.34,  0.4 ,  0.01]),\n",
       " array([ 0.2 ,  0.14,  0.35,  0.72,  0.25]),\n",
       " array([ 0.  ,  0.  ,  0.5 ,  0.2 ,  0.85])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and parse the data (make sure to convert to floats and create numpy arrays)\n",
    "mini_project_data = sc.textFile('data.csv').map(lambda line: array(map(lambda col: float(col), line.split(','))))\n",
    "            \n",
    "# Show data (only first 10 rows)\n",
    "mini_project_data.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of clusters to find\n",
    "num_clusters = 10\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "mini_project_clusters = KMeans.train(mini_project_data, num_clusters, maxIterations=10, runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200625</td>\n",
       "      <td>0.283750</td>\n",
       "      <td>0.674583</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.217667</td>\n",
       "      <td>0.224267</td>\n",
       "      <td>0.243667</td>\n",
       "      <td>0.280667</td>\n",
       "      <td>0.204333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.284118</td>\n",
       "      <td>0.608824</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.611176</td>\n",
       "      <td>0.236471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.292063</td>\n",
       "      <td>0.288281</td>\n",
       "      <td>0.285469</td>\n",
       "      <td>0.255937</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315556</td>\n",
       "      <td>0.192481</td>\n",
       "      <td>0.318889</td>\n",
       "      <td>0.662963</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.341970</td>\n",
       "      <td>0.364212</td>\n",
       "      <td>0.694545</td>\n",
       "      <td>0.659697</td>\n",
       "      <td>0.245455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.709167</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.237917</td>\n",
       "      <td>0.644708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.513562</td>\n",
       "      <td>0.262281</td>\n",
       "      <td>0.681875</td>\n",
       "      <td>0.222188</td>\n",
       "      <td>0.457188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.701579</td>\n",
       "      <td>0.314526</td>\n",
       "      <td>0.212105</td>\n",
       "      <td>0.416316</td>\n",
       "      <td>0.570526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "1  0.200625  0.283750  0.674583  0.310000  0.730000\n",
       "6  0.217667  0.224267  0.243667  0.280667  0.204333\n",
       "5  0.284118  0.608824  0.282353  0.611176  0.236471\n",
       "7  0.292063  0.288281  0.285469  0.255937  0.720000\n",
       "3  0.315556  0.192481  0.318889  0.662963  0.190000\n",
       "9  0.341970  0.364212  0.694545  0.659697  0.245455\n",
       "2  0.358500  0.709167  0.578750  0.237917  0.644708\n",
       "0  0.513562  0.262281  0.681875  0.222188  0.457188\n",
       "8  0.603100  0.502300  0.602000  0.840000  0.671000\n",
       "4  0.701579  0.314526  0.212105  0.416316  0.570526"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some details of results\n",
    "#show_clusters(mini_project_clusters)\n",
    "mini_project_week10_clusters = pd.DataFrame(mini_project_clusters.centers)\n",
    "mini_project_week10_clusters.sort([0])\n",
    "#mini_project_week10_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare this to the R output from Week9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X0.1</th>\n",
       "      <th>X0.2</th>\n",
       "      <th>X0.3</th>\n",
       "      <th>X0.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.620500</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.207593</td>\n",
       "      <td>0.252593</td>\n",
       "      <td>0.638148</td>\n",
       "      <td>0.307407</td>\n",
       "      <td>0.716296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.256842</td>\n",
       "      <td>0.177632</td>\n",
       "      <td>0.341579</td>\n",
       "      <td>0.751579</td>\n",
       "      <td>0.244211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272735</td>\n",
       "      <td>0.512324</td>\n",
       "      <td>0.315588</td>\n",
       "      <td>0.565294</td>\n",
       "      <td>0.227353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.337025</td>\n",
       "      <td>0.285325</td>\n",
       "      <td>0.213875</td>\n",
       "      <td>0.234250</td>\n",
       "      <td>0.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.373586</td>\n",
       "      <td>0.671724</td>\n",
       "      <td>0.575172</td>\n",
       "      <td>0.242759</td>\n",
       "      <td>0.669759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.470348</td>\n",
       "      <td>0.333739</td>\n",
       "      <td>0.754783</td>\n",
       "      <td>0.761739</td>\n",
       "      <td>0.454783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.527621</td>\n",
       "      <td>0.250759</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>0.217931</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.545773</td>\n",
       "      <td>0.248545</td>\n",
       "      <td>0.278636</td>\n",
       "      <td>0.453182</td>\n",
       "      <td>0.248182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.682643</td>\n",
       "      <td>0.601071</td>\n",
       "      <td>0.397143</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.662143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X0      X0.1      X0.2      X0.3      X0.4\n",
       "1   0.200000  0.221300  0.620500  0.414000  0.150000\n",
       "2   0.207593  0.252593  0.638148  0.307407  0.716296\n",
       "3   0.256842  0.177632  0.341579  0.751579  0.244211\n",
       "4   0.272735  0.512324  0.315588  0.565294  0.227353\n",
       "5   0.337025  0.285325  0.213875  0.234250  0.626500\n",
       "6   0.373586  0.671724  0.575172  0.242759  0.669759\n",
       "7   0.470348  0.333739  0.754783  0.761739  0.454783\n",
       "8   0.527621  0.250759  0.675862  0.217931  0.480000\n",
       "9   0.545773  0.248545  0.278636  0.453182  0.248182\n",
       "10  0.682643  0.601071  0.397143  0.810000  0.662143"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mini_project_week9_clusters = pd.DataFrame.from_csv(\"cluster_centers_week9.csv\")\n",
    "mini_project_week9_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get somewhat consistent results. If we look at the clusters side by side, we can see the first row of week10's clusters maps to week9's second row, and vice versa. The 8th cluster (index 0) in week 10 maps to the 8th cluster (index 8) as well. The 7th row (index 2) in week 10 matches to row 6 in week9. Some of the others are harder to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-87-49917e4930a0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-87-49917e4930a0>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    1: ,\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
