{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 / 12 - Recommendation Systems\n",
    "\n",
    "Book rcommendation using [this data set](www2.informatik.uni-freiburg.de/~cziegler/BX/). Inspiration taken from this [blog post](https://www.codementor.io/spark/tutorial/building-a-recommender-with-apache-spark-python-example-app-part1)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create PySpark context\n",
    "from pyspark import  SparkContext\n",
    "sc = SparkContext('local', 'pyspark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scription and Methodolgy\n",
    "\n",
    "The Book-Crossing dataset comprises 3 tables.\n",
    "\n",
    "* BX-Users: Contains the users. Note that user IDs (`User-ID`) have been anonymized and map to integers. \n",
    "    * For our purposes, we do not nee this file\n",
    "\n",
    "* BX-Books - Books are identified by their respective ISBN. Invalid ISBNs have already been removed from the dataset. Moreover, some content-based information is given (`Book-Title`, `Book-Author`, `Year-Of-Publication`, `Publisher`), obtained from Amazon Web Services. \n",
    "    * We load this manually to convert ISBN numbers to an internal index number to be used in the model\n",
    "\n",
    "* BX-Book-Ratings - Contains the book rating information. Ratings (`Book-Rating`) are either explicit, expressed on a scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0.\n",
    "    * This is the main file we load, which consists of more than one mullion rows. However, we filter out books that do not appear in the above BX-Books file (why this happens I am not sure). \n",
    "\n",
    "\n",
    "The following shows code for  single run of creating a model for the full data set with filtering out the implicit ratings. I also rand the code on smaller sets of the data. My methodology is to:\n",
    "\n",
    "* Create a small snippet of the data, and search for the best rank metaparameter based on validation set RMSE\n",
    "* Use this meta parameter for the medium sized data (500000 rows) and the full data (1000000+ rows) and find the test set RMSE\n",
    "* Also did the above for filtering out implitic feedback as well as not filtering it out\n",
    "\n",
    "I then also present a case of testing out the model on a new user whose only ratings are on the first two Harry Potter books.\n",
    "\n",
    "## Data Download, Extracttion\n",
    "\n",
    "Here we download the data, unzip it and then read the data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download and unzip the file\n",
    "!wget http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip\n",
    "!unzip -o BX-CSV-Dump.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into Spark RDDs\n",
    "\n",
    "Load in the book data (not sure if we need this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in book data into global dict which maps isbn to title\n",
    "isbn_to_idx = { }\n",
    "isbn_idx = 0\n",
    "book_data = open(\"BX-Books.csv\", \"r\")\n",
    "for line in book_data.readlines():\n",
    "    # Split the line and filter out quotes\n",
    "    (isbn, title) = line.split(';')[0:2]\n",
    "    isbn = isbn.replace(\"\\\"\", \"\")\n",
    "    title = title.replace(\"\\\"\", \"\")\n",
    "    \n",
    "    # Store in mapping\n",
    "    isbn_to_idx[isbn] = isbn_idx\n",
    "    isbn_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ratings data into Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1031175 ratings\n"
     ]
    }
   ],
   "source": [
    "# Read in chosen data set (min = 5000 lines, half=50000)\n",
    "\n",
    "ratings_filename = \"BX-Book-Ratings.csv\"\n",
    "# ratings_filename = \"BX-Book-Ratings.half.csv\"\n",
    "# ratings_filename = 'BX-Book-Ratings.min.csv'\n",
    "\n",
    "ratings_raw_data = sc.textFile(ratings_filename)\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0] # Used to skip the header\n",
    "\n",
    "\n",
    "# Given a row fro mthe file, filter out quotes and create dict of isbn -> index\n",
    "def convert_row(tokens):\n",
    "    (user_id, isbn, rating) = (tokens[0].replace(\"\\\"\", \"\"), \\\n",
    "                               tokens[1].replace(\"\\\"\", \"\"), \\\n",
    "                               tokens[2].replace(\"\\\"\", \"\"))\n",
    "    \n",
    "    # Filter out implicit ratings\n",
    "    if rating == 0:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    # Convert isbn to index and ...\n",
    "    global isbn_to_idx\n",
    "    if isbn in isbn_to_idx:\n",
    "        isbn_idx = isbn_to_idx[isbn]\n",
    "    else:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    # ... use it in ratings row for data frame\n",
    "    return (user_id, isbn_idx, rating)\n",
    "\n",
    "# Read in raw rating data and convert it to proper format\n",
    "ratings_data = ratings_raw_data \\\n",
    "                    .filter(lambda line: line != ratings_raw_data_header) \\\n",
    "                    .map(lambda line: line.split(\";\")) \\\n",
    "                    .map(lambda tokens: convert_row(tokens)) \\\n",
    "                    .filter(lambda tokens: tokens[0] is not None).cache()\n",
    "\n",
    "# Print ratings loaded\n",
    "print \"Loaded %d ratings\" % ratings_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1149781 BX-Book-Ratings.csv\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l \"BX-Book-Ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is only one book rated that does not exist in database of books. We skip it. \n",
    "\n",
    "Now lets create the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training, validation and test split\n",
    "training_RDD, validation_RDD, test_RDD = ratings_data.randomSplit([70, 15, 15], seed=0L)\n",
    "\n",
    "# Trim off the rating to get user-book pairs for the validation set\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Trim off the rating to get user-book pairs for the test set\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "\n",
    "Here we will use MLLib's ALS function to create models for prediction of book ratiings. We do a simple search over different rank values to do some meta-parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 36 the RMSE is 4.14137549189\n",
      "The best model was trained with rank 36\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "# Model parameters\n",
    "seed = 5L\n",
    "iterations = 10\n",
    "# iterations = 20\n",
    "regularization_parameter = 0.1\n",
    "\n",
    "# ranks = range(4, 50, 2)\n",
    "# ranks = [30] # Found by running on min data\n",
    "ranks = [36]   #  Found by running on min data, but with filtering on implicit feeback\n",
    "\n",
    "errors = [ ]\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "best_model = None\n",
    "\n",
    "# Look for best ALS model iterating over diff rank values\n",
    "for rank in ranks:\n",
    "    # Create ALS model\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations, \\\n",
    "                      lambda_=regularization_parameter)\n",
    "\n",
    "    # Come up with predictions for the validation set\n",
    "    predictions = model.predictAll(validation_for_predict_RDD)\\\n",
    "                        .map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    \n",
    "    # TODO\n",
    "    rates_and_preds = validation_RDD\\\n",
    "                        .map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\\\n",
    "                        .join(predictions)\n",
    "    \n",
    "    # Record and update the error\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors.append(error)\n",
    "    \n",
    "    # What is the current RMSE\n",
    "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
    " \n",
    "    # Error update\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "        best_model = model\n",
    "\n",
    "# Final output\n",
    "print 'The best model was trained with rank %s' % best_rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "\n",
    "Lets evaluate the best model on the held-out test set and see what the final RMSE of the model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 4.16286876454\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predictAll(test_for_predict_RDD)\\\n",
    "                        .map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\\\n",
    "                            .join(predictions)\n",
    "\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print 'For testing data the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The test set RMSE values for the mini, half and full data sets (with best rank = 30 and no filtering on implicit feedback):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+---------------+\n",
      "| Data Set (rank = 30, no filter on implicit ratings) |      RMSE     |\n",
      "+-----------------------------------------------------+---------------+\n",
      "|                Mini Data (~5000 rows)               | 5.17305247323 |\n",
      "|                 Half Data (~500,000)                | 4.36752088077 |\n",
      "|             Full Data (~1,000,000 rows)             | 4.16286876454 |\n",
      "+-----------------------------------------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Data Set (rank = 30, no filter on implicit ratings)\", \"RMSE\"]\n",
    "x.add_row([\"Mini Data (~5000 rows)\", 5.17305247323])\n",
    "x.add_row([\"Half Data (~500,000)\", 4.36752088077])\n",
    "x.add_row([\"Full Data (~1,000,000 rows)\", 4.16286876454])\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE values for the mini, half and full data sets (with best rank = 36 and filtering on implicit feedback), which we can see generally improves the RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+---------------+\n",
      "| Data Set (rank = 36, filter on implicit ratings) |      RMSE     |\n",
      "+--------------------------------------------------+---------------+\n",
      "|              Mini Data (~5000 rows)              | 5.08574829261 |\n",
      "|               Half Data (~500,000)               | 4.32793604751 |\n",
      "|           Full Data (~1,000,000 rows)            | 4.16286876454 |\n",
      "+--------------------------------------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "# Lets record our results but after filtering out implicit feedback\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Data Set (rank = 36, filter on implicit ratings)\", \"RMSE\"]\n",
    "x.add_row([\"Mini Data (~5000 rows)\", 5.08574829261])\n",
    "x.add_row([\"Half Data (~500,000)\", 4.32793604751])\n",
    "x.add_row([\"Full Data (~1,000,000 rows)\", 4.16286876454])\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Test Some Predicitions\n",
    "\n",
    "First, lets find some books to rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'0590353403', u\"Harry Potter and the Sorcerer's Stone (Book 1)\")]\n",
      "[(u'0439064872', u'Harry Potter and the Chamber of Secrets (Book 2)')]\n",
      "[(u'0439136350', u'Harry Potter and the Prisoner of Azkaban (Book 3)')]\n"
     ]
    }
   ],
   "source": [
    "print books_data.filter(lambda x: x[0]=='0590353403').take(1) \n",
    "print books_data.filter(lambda x: x[0]=='0439064872').take(1)\n",
    "print books_data.filter(lambda x: x[0]=='0439136350').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to determine a user ID that is not used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"User-ID\";\"Location\";\"Age\"\r",
      "\r\n",
      "\"1\";\"nyc, new york, usa\";NULL\r",
      "\r\n",
      "\"2\";\"stockton, california, usa\";\"18\"\r",
      "\r\n",
      "\"3\";\"moscow, yukon territory, russia\";NULL\r",
      "\r\n",
      "\"4\";\"porto, v.n.gaia, portugal\";\"17\"\r",
      "\r\n",
      "\"5\";\"farnborough, hants, united kingdom\";NULL\r",
      "\r\n",
      "\"6\";\"santa monica, california, usa\";\"61\"\r",
      "\r\n",
      "\"7\";\"washington, dc, usa\";NULL\r",
      "\r\n",
      "\"8\";\"timmins, ontario, canada\";NULL\r",
      "\r\n",
      "\"9\";\"germantown, tennessee, usa\";NULL\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head BX-Users.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User ID 0 looks fine. Now, we will create two ratings on the first two harry potter books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indies for potter books 2810 3460 3840\n"
     ]
    }
   ],
   "source": [
    "print \"Indies for potter books\", \\\n",
    "        isbn_to_idx[\"0590353403\"], \\\n",
    "        isbn_to_idx[\"0439064872\"], \\\n",
    "        isbn_to_idx[\"0439136350\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For user 0, create two ratings for the first two books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 2810, 10), (0, 3460, 10)]\n"
     ]
    }
   ],
   "source": [
    "new_user_id = 0\n",
    "new_user_ratings = [\n",
    "     (new_user_id, 2810, 10),\n",
    "     (new_user_id, 3460, 10), \n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print 'New user ratings: %s' % new_user_ratings_RDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new model based on the training data but with our new user recommendations (via RDD union) and with the best_rank calculated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_RDD_with_new_ratings = training_RDD.union(new_user_ratings_RDD)\n",
    "\n",
    "\n",
    "new_ratings_model = ALS.train(training_RDD_with_new_ratings, best_rank, seed=seed, \n",
    "                              iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what do we predict as the rating for this user for the third potter book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.075894409195863"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ratings_model.predict(new_user_id, 3840)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details / Issues\n",
    "\n",
    "These results aren;t so great. While we get better results using more data, the RMSE are pretty high, considering the scale is from 1 - 10. \n",
    "\n",
    "One issue that that it would be great to merge books: some books in the data set are essentially the same book, but a different edition, or a different printing, or paperback versus hardcover. They represent the same 'thing' in the world, and having some way of merging them would allow for better recommendations. \n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
