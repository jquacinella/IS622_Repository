---
title: "Week2 - Discussion Post (KMeans)"
author: "James Quacinella"
output: pdf_document
---

# Simple Example 

To start, lets make sure hadoop and RHadoop are working. Using an exmaple from https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md, I will first import the various libraries and then, for now, turn hadoop off.

```{r results='hide'}
library(rJava)
library(rhdfs)
hdfs.init()
library(rmr2)

rmr.options(backend = "local")
```

## Simple MapReduce with no Hadoop

Below, we create some data from the Binomial distribution and look to map reduce to tell us the various counts of numbers generated. I added print statements to help me understand how things are being passed around:

```{r}
groups = rbinom(32, n = 50, prob = 0.4)
groups_dfs_local = to.dfs(groups)

from.dfs(
  mapreduce(
    input = groups_dfs_local, 
    map = function(k, v) {
      print("Value map:")
      print(v);
      keyval(v, 1);
    }, 
    reduce = 
      function(k, vv) {
        print(paste("Key reduce: ", k));
        print("Values reduce:")
        print(vv);
        keyval(k, length(vv)) ;
      }
  )
)
```

## Simple MapReduce with Hadoop

Lets turn Hadoop back on and watch it work:

```{r cache=TRUE}
rmr.options(backend = "hadoop")
groups_dfs = to.dfs(groups)

from.dfs(
  mapreduce(
    input = groups_dfs, 
    map = function(k, v) {
      keyval(v, 1);
    }, 
    reduce = 
      function(k, vv) {
        keyval(k, length(vv)) ;
      }
  )
)
```

NOTE: the output is sorted, proving it went to hadoop even though the output does not show here. Also note that hadoop has a high startup cost.

# K-Means Example

Continuing with the example, we will perform K-Means clustering on a sample data set. First, lets grab the data:

```{r cache=TRUE}
green_taxi_data_csv <- read.csv("~/Code/Masters/IS622/Week2/green_tripdata_2015-01.trimmed.csv")
green_taxi_data <- as.matrix(green_taxi_data_csv[,c("Trip_distance","Fare_amount")])
```

Notice that I am trimming the data to the columns that I want to cluster. I did not choose the obvious thing to cluster, the pickup or dropoff locations, since they didn't seem to have much variation. This makes sense since the green taxis work in a much more limited area. I took inspiration from Rohan's code.

Also notice that I am using only a trimmed version of the input file, of 1000 lines. This makes hadoop run on my machine, since its memory strapped.

## Without Hadoop

Now lets load it into the DFS, but lets stay local for now:

```{r}
rmr.options(backend = "local")
green_taxi_data_dfs_local <- to.dfs(green_taxi_data)
```

Next, we will setup our map-reduce job:

```{r}
## @knitr kmeans-signature
kmeans.mr = 
  function(
    P, 
    num.clusters, 
    num.iter, 
    combine, 
    in.memory.combine) {
## @knitr kmeans-dist.fun
    dist.fun = 
      function(C, P) {
        apply(
          C,
          1, 
          function(x) 
            colSums((t(P) - x)^2))}
## @knitr kmeans.map
    kmeans.map = 
      function(., P) {
        nearest = {
          if(is.null(C)) 
            sample(
              1:num.clusters, 
              nrow(P), 
              replace = TRUE)
          else {
            D = dist.fun(C, P)
            nearest = max.col(-D)}}
        if(!(combine || in.memory.combine))
          keyval(nearest, P) 
        else 
          keyval(nearest, cbind(1, P))}
## @knitr kmeans.reduce
    kmeans.reduce = {
      if (!(combine || in.memory.combine) ) 
        function(., P) 
          t(as.matrix(apply(P, 2, mean)))
      else 
        function(k, P) 
          keyval(
            k, 
            t(as.matrix(apply(P, 2, sum))))}
## @knitr kmeans-main-1  
    C = NULL
    for(i in 1:num.iter ) {
      C = 
        values(
          from.dfs(
            mapreduce(
              P, 
              map = kmeans.map,
              reduce = kmeans.reduce)))
      if(combine || in.memory.combine)
        C = C[, -1]/C[, 1]
## @knitr end
#      points(C, col = i + 1, pch = 19)
## @knitr kmeans-main-2
      if(nrow(C) < num.clusters) {
        C = 
          rbind(
            C,
            matrix(
              rnorm(
                (num.clusters - 
                   nrow(C)) * nrow(C)), 
              ncol = nrow(C)) %*% C) }}
        C}
## @knitr end
```

Lets run the map-reduce job and see the results:

```{r cache=TRUE}
kmeans.mr(
      green_taxi_data_dfs_local,
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.memory.combine = FALSE)
```


## With Hadoop

Lets load the data now into HDFS:

```{r}
rmr.options(backend = "hadoop")
green_taxi_data_dfs <- to.dfs(green_taxi_data)
```

Lets re-run the map-reduce job and see the results:

```{r cache=TRUE}
kmeans.mr(
      green_taxi_data_dfs,
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.memory.combine = FALSE)
```